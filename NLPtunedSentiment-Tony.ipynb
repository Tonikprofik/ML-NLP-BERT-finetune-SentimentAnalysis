{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ae4abb-a0d1-4646-9c02-e7202111647f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2023.7.22)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting torch\n",
      "  Using cached torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.7.1)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.2.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.2)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.48.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.21.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.29.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.11.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd84eea-a34b-4974-8b35-91c44a088df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")  # many other tasks are available\n",
    "result = classifier(\"The actors were very convincing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee53a35-59c0-4ba3-a3a7-8ac4b72f386a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9564545750617981},\n",
       " {'label': 'NEGATIVE', 'score': 0.9747399091720581},\n",
       " {'label': 'NEGATIVE', 'score': 0.9941459894180298}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"I am from Czech\", \"I am from Vietnam\", \"I am lesbian\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b88cb89-99dd-4443-8e62-b95732da6574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# determine relationship - entailment, contradiction, neutral - MultiGenre NL Inference task\n",
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
    "classifier_mnli(\"She loves me. [SEP] She loves me not.\")\n",
    "\n",
    "distiltunedmli_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "distilberttunedmli_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce92fec-b661-4fe4-8885-d6e89aa4c40b",
   "metadata": {},
   "source": [
    "# distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f06f39e5-28d4-42d4-b632-2107377537f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "# AutoTokenizer loads the tokenizer for the 'distilbert-base-uncased' model. \n",
    "#TFAutoModelForSequenceClassification\n",
    "#This model has been pre-trained on a large corpus of text. can be fine-tuned for sequence classification tasks.\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32af5cb-aed7-45b2-81ce-3f458d49ccfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentiment analysis with labeled reviews - distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aad317dd-9d1b-4967-9a6f-da4df713cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  So there is no way for me to plug it in here i...         0\n",
      "1                        Good case, Excellent value.         1\n",
      "2                             Great for the jawbone.         1\n",
      "3  Tied to charger for conversations lasting more...         0\n",
      "4                                  The mic is great.         1\n"
     ]
    }
   ],
   "source": [
    "# head of data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Initialize a DataFrame to hold all the data\n",
    "df = pd.DataFrame(columns=['review', 'sentiment'])\n",
    "\n",
    "# Load the data from each .txt file\n",
    "for filename in os.listdir('sentimentlabeled'):\n",
    "    if filename.endswith('.txt'):\n",
    "        path = os.path.join('sentimentlabeled', filename)\n",
    "        data = pd.read_csv(path, sep='\\t', names=['review', 'sentiment'])\n",
    "        df = pd.concat([df, data])\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32a11dfc-6e7f-4bc1-881e-30d18405164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the reviews\n",
    "inputs = distilbert_tokenizer(df['review'].tolist(), truncation=True, padding='max_length', max_length=128, return_tensors='tf')\n",
    "\n",
    "# Convert inputs to numpy arrays\n",
    "input_ids = inputs['input_ids'].numpy()\n",
    "attention_mask = inputs['attention_mask'].numpy()\n",
    "\n",
    "# Convert sentiment labels to integers\n",
    "labels = df['sentiment'].tolist()  # assuming the sentiment labels are already integers\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.2)\n",
    "train_masks, val_masks, _, _ = train_test_split(attention_mask, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5e6be-ace5-4cbc-b1a9-2cbf72c8e599",
   "metadata": {},
   "source": [
    "## DistilBERT-MLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40243a31-4ecc-41d1-bf9f-984061881e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "138/138 [==============================] - 57s 242ms/step - loss: 0.4046 - accuracy: 0.8212 - val_loss: 0.2786 - val_accuracy: 0.8709\n",
      "Epoch 2/3\n",
      "138/138 [==============================] - 31s 221ms/step - loss: 0.1748 - accuracy: 0.9308 - val_loss: 0.3625 - val_accuracy: 0.8727\n",
      "Epoch 3/3\n",
      "138/138 [==============================] - 30s 220ms/step - loss: 0.0742 - accuracy: 0.9732 - val_loss: 0.3882 - val_accuracy: 0.8618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda4f6c5090>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "# Initialize the model\n",
    "distiltunedmli_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "distilberttunedmli_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "val_labels = tf.convert_to_tensor(val_labels)\n",
    "\n",
    "# Compile the model sets the optimizer for the training process. \n",
    "#The optimizer is the algorithm used to change the attributes of the neural network such as weights and learning rate in order to reduce the losses. \n",
    "# Adam is a commonly used optimizer that adapts the learning rate for each weight individually. \n",
    "#The learning rate of `5e-5` is a commonly used value for transformer models.\n",
    "distilberttunedmli_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "#sets the loss function, model will try to minimize during training.\n",
    "# SparseCategoricalCrossentropy is a common loss function for multi-class classification problems.\n",
    "# The `from_logits=True` argument means that the function should treat the model's output as unnormalized log probabilities (logits) rather than probabilities.\n",
    "#`metrics=['accuracy']`: sets the metric evaluated during training and testing. Accuracy is a common metric for classification problems, and it simply measures the proportion of correct predictions.\n",
    "\n",
    "# Train the model\n",
    "distilberttunedmli_model.fit([train_inputs, train_masks], train_labels, validation_data=([val_inputs, val_masks], val_labels), batch_size=16, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8caf865-6122-4073-906d-7d1790359143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the reviews\n",
    "inputs = distilbert_tokenizer(df['review'].tolist(), truncation=True, padding='max_length', max_length=128, return_tensors='tf')\n",
    "\n",
    "# Convert inputs to numpy arrays\n",
    "input_ids = inputs['input_ids'].numpy()\n",
    "attention_mask = inputs['attention_mask'].numpy()\n",
    "\n",
    "# Convert sentiment labels to integers\n",
    "labels = df['sentiment'].tolist()  # assuming the sentiment labels are already integers\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.2)\n",
    "train_masks, val_masks, _, _ = train_test_split(attention_mask, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da058358-7811-4392-a95c-a1b1b2f98366",
   "metadata": {},
   "source": [
    "## training from distilBert base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "930eaf49-1547-47b4-b8e2-e5404cdae556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "138/138 [==============================] - 58s 244ms/step - loss: 0.3996 - accuracy: 0.8057 - val_loss: 0.3015 - val_accuracy: 0.8582\n",
      "Epoch 2/3\n",
      "138/138 [==============================] - 30s 221ms/step - loss: 0.1934 - accuracy: 0.9240 - val_loss: 0.3045 - val_accuracy: 0.8600\n",
      "Epoch 3/3\n",
      "138/138 [==============================] - 31s 221ms/step - loss: 0.1060 - accuracy: 0.9627 - val_loss: 0.4039 - val_accuracy: 0.8564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda7079d710>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data prepared, proceeding to fine tune distilBert\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "# Initialize the model\n",
    "distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "val_labels = tf.convert_to_tensor(val_labels)\n",
    "\n",
    "# Compile the model sets the optimizer for the training process. \n",
    "#The optimizer is the algorithm used to change the attributes of the neural network such as weights and learning rate in order to reduce the losses. \n",
    "# Adam is a commonly used optimizer that adapts the learning rate for each weight individually. \n",
    "#The learning rate of `5e-5` is a commonly used value for transformer models.\n",
    "distilbert_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "#sets the loss function, model will try to minimize during training.\n",
    "# SparseCategoricalCrossentropy is a common loss function for multi-class classification problems.\n",
    "# The `from_logits=True` argument means that the function should treat the model's output as unnormalized log probabilities (logits) rather than probabilities.\n",
    "#`metrics=['accuracy']`: sets the metric evaluated during training and testing. Accuracy is a common metric for classification problems, and it simply measures the proportion of correct predictions.\n",
    "\n",
    "# Train the model\n",
    "distilbert_model.fit([train_inputs, train_masks], train_labels, validation_data=([val_inputs, val_masks], val_labels), batch_size=16, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed34ac07-2031-4c4f-96c1-d39b5314ff5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tunedmli_tokenizer/tokenizer_config.json',\n",
       " 'tunedmli_tokenizer/special_tokens_map.json',\n",
       " 'tunedmli_tokenizer/vocab.txt',\n",
       " 'tunedmli_tokenizer/added_tokens.json',\n",
       " 'tunedmli_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned DistilBERT model, checkpoint\n",
    "distilbert_model.save_pretrained('tony_tuned_distilbert_model')\n",
    "distilbert_tokenizer.save_pretrained('tony_tuned_distilbert_tokenizer')\n",
    "\n",
    "distilberttunedmli_model.save_pretrained('tunedmli_model')\n",
    "distiltunedmli_tokenizer.save_pretrained('tunedmli_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d52af4-194d-4725-80ba-fe6a902cee6c",
   "metadata": {},
   "source": [
    "## Load checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1121e99-6664-4c6c-a1d6-128f1c87fa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at tony_tuned_distilbert_model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_377']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at tony_tuned_distilbert_model and are newly initialized: ['dropout_417']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('tony_tuned_distilbert_model')\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained('tony_tuned_distilbert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d86b2-2966-44b1-a3e4-04bd5fa36dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at tony_tuned_distilbert_model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_377']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at tony_tuned_distilbert_model and are newly initialized: ['dropout_417']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "distilberttunedmli_model.save_pretrained('tunedmli_model')\n",
    "distiltunedmli_tokenizer.save_pretrained('tunedmli_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b7e95-4c44-415c-96d0-e53a453e1f2b",
   "metadata": {},
   "source": [
    "## Model eval - tuned mli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "90ff16b9-a5ec-4e1c-a9e7-ab25f041cb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "tf.Tensor(\n",
      "[[1.2880618e-03 9.9847311e-01 2.3870940e-04]\n",
      " [1.8707142e-03 9.9784184e-01 2.8737498e-04]\n",
      " [9.9959046e-01 2.3521387e-04 1.7433712e-04]\n",
      " [9.9702990e-01 2.4592008e-03 5.1087979e-04]\n",
      " [2.4574117e-03 9.9711823e-01 4.2431121e-04]], shape=(5, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have a new review\n",
    "new_review = [\"The hotel was excellent!\", \"The room was very clean.\", \"The food was terrible.\", \"I wouldn't recommend this place.\", \"Best hotel I've ever stayed at!\"]\n",
    "# Make sure the review is a list of strings\n",
    "if isinstance(new_review, str):\n",
    "    new_review = [new_review]\n",
    "    \n",
    "# Tokenize the new review\n",
    "new_inputs = distiltunedmli_tokenizer(new_review, truncation=True, padding=True, return_tensors='tf')\n",
    "\n",
    "# Make a prediction\n",
    "predictions = distilberttunedmli_model.predict([new_inputs['input_ids'], new_inputs['attention_mask']])\n",
    "\n",
    "# The predictions are logits (unnormalized scores). softmax function, to get probabilities\n",
    "probabilities = tf.nn.softmax(predictions[0], axis=-1)\n",
    "\n",
    "# Print the probabilities\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c258fc7-0224-4aba-ad01-90ae130115e4",
   "metadata": {},
   "source": [
    "#### fine-tuned on the Multi-Genre Natural Language Inference (MNLI) task. The MNLI task is a three-class classification problem, where each instance is classified as entailment, contradiction, or neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "388e7e26-4e8e-410c-a56b-143bb44d032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 4s 131ms/step\n",
      "AUC-ROC: 0.952811404906021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get the model's predicted probabilities for the positive class\n",
    "probabilities = distilberttunedmli_model.predict([val_inputs, val_masks])[0]\n",
    "probabilities = tf.nn.softmax(probabilities, axis=-1)\n",
    "positive_probabilities = probabilities[:, 1]\n",
    "\n",
    "# Compute the AUC-ROC\n",
    "auc_roc = roc_auc_score(val_labels, positive_probabilities)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8c24335-ddf5-4a9d-9c6d-92d2b52d85fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 2s 133ms/step\n",
      "F1 Score: 0.8606480938416422\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Get the model's predicted probabilities\n",
    "probabilities = distilberttunedmli_model.predict([val_inputs, val_masks])[0]\n",
    "probabilities = tf.nn.softmax(probabilities, axis=-1)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "class_predictions = np.argmax(probabilities, axis=-1)\n",
    "\n",
    "# Compute the F1 score - harmonic mean of precision and recall\n",
    "f1 = f1_score(val_labels, class_predictions, average='weighted')\n",
    "\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d889ef-de0d-4427-953c-90cecc05d984",
   "metadata": {},
   "source": [
    "## Model eval finetuned distilbert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258dd6b-9500-46a2-bdbc-d407984f011a",
   "metadata": {},
   "source": [
    "### class `0` for negative sentiment and class `1` for positive sentiment. \n",
    "the model is predicting with high confidence (approximately 99.63%) that the review \"The hotel was excellent!\" has a positive sentiment. \n",
    "The low probability (approximately 0.37%) corresponds to the negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d6468-1ca0-4160-b69a-fa9d1113e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n",
      "tf.Tensor(\n",
      "[[0.0036537  0.9963463 ]\n",
      " [0.9934685  0.00653151]\n",
      " [0.9972819  0.00271812]\n",
      " [0.06982092 0.93017906]\n",
      " [0.00551693 0.99448305]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have a new review\n",
    "new_review = [\"The hotel was excellent!\", \"The room was very clean.\", \"The food was terrible.\", \"I wouldn't recommend this place.\", \"Best hotel I've ever stayed at!\"]\n",
    "# Make sure the review is a list of strings\n",
    "if isinstance(new_review, str):\n",
    "    new_review = [new_review]\n",
    "    \n",
    "# Tokenize the new review\n",
    "new_inputs = distilbert_tokenizer(new_review, truncation=True, padding=True, return_tensors='tf')\n",
    "\n",
    "# Make a prediction\n",
    "predictions = distilbert_model.predict([new_inputs['input_ids'], new_inputs['attention_mask']])\n",
    "\n",
    "# The predictions are logits (unnormalized scores). softmax function, to get probabilities\n",
    "probabilities = tf.nn.softmax(predictions[0], axis=-1)\n",
    "\n",
    "# Print the probabilities\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546dc3c6-4f29-4313-bf1f-d80ee9861d72",
   "metadata": {},
   "source": [
    "notice `room was very clean` is scored as negative. False negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1af81c0a-460a-4226-a48b-5246732637f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 4s 130ms/step\n",
      "AUC-ROC: 0.9463204842306466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get the model's predicted probabilities for the positive class\n",
    "probabilities = distilbert_model.predict([val_inputs, val_masks])[0]\n",
    "probabilities = tf.nn.softmax(probabilities, axis=-1)\n",
    "positive_probabilities = probabilities[:, 1]\n",
    "\n",
    "# Compute the AUC-ROC\n",
    "auc_roc = roc_auc_score(val_labels, positive_probabilities)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "338ded86-a3de-4da6-86c9-09159bd48a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 2s 131ms/step\n",
      "F1 Score: 0.8563470172253012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Get the model's predicted probabilities\n",
    "probabilities = distilbert_model.predict([val_inputs, val_masks])[0]\n",
    "probabilities = tf.nn.softmax(probabilities, axis=-1)\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "class_predictions = np.argmax(probabilities, axis=-1)\n",
    "\n",
    "# Compute the F1 score - harmonic mean of precision and recall\n",
    "f1 = f1_score(val_labels, class_predictions, average='weighted')\n",
    "\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da8525-27e0-4c4a-9330-d6c96a5f6c4a",
   "metadata": {},
   "source": [
    "# Electra model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c461e2c-d852-47c1-b662-7c3ae2d596e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-base-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "138/138 [==============================] - 112s 466ms/step - loss: 0.4331 - accuracy: 0.7925 - val_loss: 0.2036 - val_accuracy: 0.9073\n",
      "Epoch 2/3\n",
      "138/138 [==============================] - 60s 437ms/step - loss: 0.2292 - accuracy: 0.9140 - val_loss: 0.3036 - val_accuracy: 0.8873\n",
      "Epoch 3/3\n",
      "138/138 [==============================] - 60s 434ms/step - loss: 0.1424 - accuracy: 0.9540 - val_loss: 0.3933 - val_accuracy: 0.9145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda558ebad0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFElectraForSequenceClassification, ElectraTokenizer\n",
    "\n",
    "# Initialize the tokenizer and the model\n",
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "electra_model = TFElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "val_labels = tf.convert_to_tensor(val_labels)\n",
    "\n",
    "# Compile the model\n",
    "electra_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "electra_model.fit([train_inputs, train_masks], train_labels, validation_data=([val_inputs, val_masks], val_labels), batch_size=16, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b6fada76-87b6-40c2-93df-f7641080c34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('electra_tokenizer/tokenizer_config.json',\n",
       " 'electra_tokenizer/special_tokens_map.json',\n",
       " 'electra_tokenizer/vocab.txt',\n",
       " 'electra_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned ELECTRA model\n",
    "electra_model.save_pretrained('electra_model')\n",
    "electra_tokenizer.save_pretrained('electra_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cec875-ed43-427d-9dc0-4a13239b78bd",
   "metadata": {},
   "source": [
    "### Load Electra checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aae9dc-d846-4062-bc37-608d52760934",
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_tokenizer = ElectraTokenizer.from_pretrained('electra_tokenizer')\n",
    "electra_model = TFElectraForSequenceClassification.from_pretrained('electra_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5fdf5814-7bb0-40a7-924e-a4ce9d2a8b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "tf.Tensor(\n",
      "[[0.0030535  0.9969465 ]\n",
      " [0.00287638 0.99712366]\n",
      " [0.97472817 0.02527181]\n",
      " [0.96255565 0.0374443 ]\n",
      " [0.00304213 0.9969579 ]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "new_review = [\"The hotel was excellent!\", \"The room was very clean.\", \"The food was terrible.\", \"I wouldn't recommend this place.\", \"Best hotel I've ever stayed at!\"]\n",
    "\n",
    "# Make sure the review is a list of strings\n",
    "if isinstance(new_review, str):\n",
    "    new_review = [new_review]\n",
    "\n",
    "# Tokenize the new review\n",
    "new_inputs = electra_tokenizer(new_review, truncation=True, padding=True, return_tensors='tf')\n",
    "\n",
    "# Make a prediction\n",
    "predictions = electra_model.predict([new_inputs['input_ids'], new_inputs['attention_mask']])\n",
    "\n",
    "# The predictions are logits (unnormalized scores). Apply the softmax function to get probabilities\n",
    "probabilities = tf.nn.softmax(predictions[0], axis=-1)\n",
    "\n",
    "# Print the probabilities\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9712d-004d-4087-8e92-854cfd96f9bf",
   "metadata": {},
   "source": [
    "## Model evaluation tuned Elektra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e73d06fd-86de-4d5a-a807-de4780867da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 8s 265ms/step\n",
      "AUC-ROC: 0.9665233089094192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get the model's predicted probabilities for the positive class\n",
    "probabilities = electra_model.predict([val_inputs, val_masks])[0]\n",
    "probabilities = tf.nn.softmax(probabilities, axis=-1)\n",
    "positive_probabilities = probabilities[:, 1]\n",
    "\n",
    "# Compute the AUC-ROC\n",
    "auc_roc = roc_auc_score(val_labels, positive_probabilities)\n",
    "\n",
    "print('AUC-ROC:', auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cf23a19-81a3-4106-b9ce-abe3d752bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 5s 261ms/step\n",
      "F1 Score: 0.9137907653583428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Get the model's predictions\n",
    "predictions = electra_model.predict([val_inputs, val_masks])[0]\n",
    "predictions = tf.nn.softmax(predictions, axis=-1)\n",
    "\n",
    "# Convert the probabilities to class labels\n",
    "predicted_labels = tf.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute the F1 score\n",
    "f1 = f1_score(val_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f633c83-3376-446a-8de4-2ae6c96541b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.11 (Local)",
   "language": "python",
   "name": "local-tf2-2-11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
